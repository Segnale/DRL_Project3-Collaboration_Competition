{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to my coding environment for multi agent train for the Udacity project of Collaboration and Competition. The aim of the code is to train a competing agents playing in the a simple Unity enviroment of a tennis court.\n",
    "\n",
    "Follow the instructions below to get started!\n",
    "\n",
    "\n",
    "The first step makes available some utility libreries used throughout the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utilities import Plotting\n",
    "import random\n",
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "import progressbar as pb\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment\n",
    "\n",
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. State and Action Spaces\n",
    "\n",
    "The code in the cell below prints some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "<class 'numpy.ndarray'>\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print(type(states))\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Actions in the Environment to Initialize the Replay Buffer\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Running on: ' + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Buffer fillup: 100% |###########################################| Time: 0:00:19\n"
     ]
    }
   ],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "buffer_size = 10000\n",
    "batch_size = 256 \n",
    "\n",
    "replay_buffer = ReplayBuffer(device, buffer_size, batch_size)\n",
    "\n",
    "widget = ['Buffer fillup: ', pb.Percentage(), ' ',pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=buffer_size).start()\n",
    "\n",
    "for i in range(buffer_size):\n",
    "    actions =  (np.random.rand(action_size,action_size)*2)-1 # btwn -1..1\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    \n",
    "    replay_buffer.add(states.reshape(-1), actions.reshape(-1), np.max(rewards), next_states.reshape(-1), np.max(dones))\n",
    "    states = next_states\n",
    "    timer.update(i+1)\n",
    "    \n",
    "    if True in dones:\n",
    "        env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "\n",
    "timer.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Agent and Hyperparameters\n",
    "\n",
    "The agent class is coded in the `my_agent.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from model import Actor, Critic\n",
    "\n",
    "# The agent implementation is common and use the experiance of both the two tennis players\n",
    "state_size = state_size*num_agents\n",
    "action_size = action_size*num_agents\n",
    "\n",
    "# agent\n",
    "agent = Agent(\n",
    "    device, state_size, action_size,\n",
    "    Actor, Critic,\n",
    "    lrate_critic=1e-3, lrate_actor=1e-4,\n",
    "    tau=0.01, gamma=0.99,\n",
    "    exploration_mu=0.0, exploration_theta=0.15, exploration_sigma=0.20,\n",
    "    seed=np.random.randint(1000),\n",
    "    weight_decay=0, noise_decay=0.99995\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. It's Turn of Train the Agent\n",
    "\n",
    "Now it's your turn to train the agent to solve the environment!  The enviroment is considered solved when the agent achieves an avarage score of 0.5 points over 100 episodes.\n",
    "While training the agent, you can see the progress in an online tred.\n",
    "\n",
    "The training follow few main steps, for each episode:\n",
    "- the actor model of the agent run actions on the enviroment with some added noise to ensure exploration. The `OUNoise` Class is used for this purpose.\n",
    "- additional informations from the enviroment are retrieved after the action of the agent (reward, next_state, etc.) and are piled into the **Replay Buffer**. \n",
    "- experiances are extracted from the **Replay Buffer** and used to:\n",
    "    - calculate the TD target\n",
    "    - soft-update the critic model\n",
    "    - soft-update the actor model\n",
    "- The results are collected and showed and if the avarage score passes the 0.5 points and improve from the previous episodes, the models are checkpointed.\n",
    "- At the end of the training the trend with the progress is saved in the folder `Results\\`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "episodes = 1000\n",
    "steps = 2000\n",
    "\n",
    "scores = deque(maxlen=100)\n",
    "PScores = []\n",
    "\n",
    "# Score Trend Initialization\n",
    "plot = Plotting(\n",
    "    title ='Learning Process',\n",
    "    y_label = 'Score',\n",
    "    x_label = 'Episode #',\n",
    "    x_range = 250,\n",
    ")\n",
    "plot.show()\n",
    "\n",
    "# Progress bar o terminal\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ' ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()\n",
    "\n",
    "buffer_fed_n = 0\n",
    "last_saved = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # Reset the OUNoise state\n",
    "    agent.reset_episode()\n",
    "    \n",
    "    # Enviroment initialization and initial state\n",
    "    env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    score = np.zeros(num_agents)\n",
    "    #dones = [False for done in dones]\n",
    "    \n",
    "    #while not True in dones:\n",
    "    for step_i in range(steps):\n",
    "        \n",
    "        # Agent Interact with the enviroment\n",
    "        actions = agent.act(states.reshape(-1))\n",
    "        \n",
    "        # Enviroment response\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        \n",
    "        #####next_state, reward, done = env.step(action.reshape(2,-1))\n",
    "        \n",
    "        # Interactions are collected in the replay buffer\n",
    "        replay_buffer.add(states.reshape(-1), actions.reshape(-1), np.max(rewards), next_states.reshape(-1), np.max(dones))\n",
    "        buffer_fed_n = buffer_fed_n + 1\n",
    "        \n",
    "        # Agent learn step\n",
    "        if buffer_fed_n >= batch_size:\n",
    "            agent.learn_step(replay_buffer)\n",
    "        \n",
    "        # Shift to next experiance step\n",
    "        states = next_states\n",
    "        score += rewards                     # Episode score builtup\n",
    "        \n",
    "        # Episode end condition\n",
    "        if True in dones:\n",
    "            break\n",
    "    # collect  episode results\n",
    "    scores.append(score.max())\n",
    "    mean = np.sum(scores)/100\n",
    "    #if (episode+1)%100 ==0 :\n",
    "    #    print(\"Episode: {0:d}, Score: {1:f}\".format(episode+1,mean))\n",
    "    timer.update(episode+1)\n",
    "    PScores.append(mean)\n",
    "    plot.Update(list(range(episode+1)),PScores)\n",
    "    #summary += f', Score: {mean:.3f}'\n",
    "    # writer.add_scalar('data/score', mean, ep_i)\n",
    "    if mean > 0.50 and mean > last_saved:\n",
    "        last_saved = mean\n",
    "        agent.save('saved/trained_model.ckpt')\n",
    "    \n",
    "timer.finish()\n",
    "\n",
    "pdb.set_trace()\n",
    "\n",
    "# Save Training Trend\n",
    "end_plot = Plotting(\n",
    "    title ='Learning Process',\n",
    "    y_label = 'Score',\n",
    "    x_label = 'Episode #',\n",
    "    x_values = list(range(episode-(100-2))),\n",
    "    y_values = PScores\n",
    ")\n",
    "end_plot.save('Results/Training.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. See the Agent Performs!\n",
    "\n",
    "Now that the agent is trained you can see it in action!\n",
    "\n",
    "After loading the checkpoint an episode is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "agent = Agent(device, state_size, action_size, Actor, Critic)\n",
    "agent.restore('saved/trained_model.ckpt')\n",
    "\n",
    "scores = []\n",
    "worked_out_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_batch = 7\n",
    "episodes = worked_out_episodes + episode_batch \n",
    "for episode in range(worked_out_episodes, episodes):\n",
    "    score = np.zeros(2)\n",
    "    \n",
    "    # Enviroment initialization and initial state\n",
    "    env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Agent Interact with the enviroment\n",
    "        actions = agent.act(states.reshape(-1), learn=False)\n",
    "        # Enviroment response\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        \n",
    "        #next_state, reward, done = env.step(action.reshape(2, -1))\n",
    "        score += rewards\n",
    "        states = next_states\n",
    "        if True in dones:\n",
    "            break\n",
    "            \n",
    "    scores.append(np.max(score))\n",
    "    print('Episode: {0}/{1}, Score Agt. #1: {2[0]:.3f}, Score Agt. #2: {2[1]:.3f}'.format(episode+1, episodes, score))\n",
    "\n",
    "print('Max score on {0} episodes: {1}'.format(episodes, np.max(score)))\n",
    "worked_out_episodes = episode+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run again the above cell if you want to play more tennis.\n",
    "Otherwise, close the enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
