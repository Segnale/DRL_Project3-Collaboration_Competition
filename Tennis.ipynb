{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to my coding environment for multi agent train for the Udacity project of Collaboration and Competition. The aim of the code is to train a competing agents playing in the a simple Unity enviroment of a tennis court.\n",
    "\n",
    "Follow the instructions below to get started!\n",
    "\n",
    "\n",
    "The first step makes available some utility libreries used throughout the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utilities import Plotting\n",
    "import random\n",
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "import progressbar as pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment\n",
    "\n",
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. State and Action Spaces\n",
    "\n",
    "The code in the cell below prints some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "<class 'numpy.ndarray'>\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print(type(states))\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Actions in the Environment to Initialize the Replay Buffer\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Buffer fillup: 100% |###########################################| Time: 0:00:25\n"
     ]
    }
   ],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "buffer_size = 15000\n",
    "batch_size = 256 \n",
    "\n",
    "replay_buffer = ReplayBuffer(device, buffer_size, batch_size)\n",
    "\n",
    "widget = ['Buffer fillup: ', pb.Percentage(), ' ',pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=buffer_size).start()\n",
    "\n",
    "for i in range(buffer_size):\n",
    "    actions =  (np.random.rand(action_size,action_size)*2)-1 # btwn -1..1\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    \n",
    "    replay_buffer.add(states.shape[0], actions.shape[0], np.max(rewards), next_states.shape[0], np.max(dones))\n",
    "    states = next_states\n",
    "    timer.update(i+1)\n",
    "    \n",
    "    if True in dones:\n",
    "        env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "\n",
    "timer.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Agent and Hyperparameters\n",
    "\n",
    "The agent class is coded in the `my_agent.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n"
     ]
    }
   ],
   "source": [
    "from agent_jn import Agent\n",
    "from model import Actor, Critic\n",
    "\n",
    "# The agent implementation is common and use the experiance of both the two tennis players\n",
    "state_size = state_size*num_agents\n",
    "action_size = action_size*num_agents\n",
    "\n",
    "# agent\n",
    "agent = Agent(\n",
    "    state_size, action_size, Actor, Critic,\n",
    "    lrate_critic=1e-3,\n",
    "    lrate_actor=1e-4,\n",
    "    tau=0.01,\n",
    "    buffer_size=1e6,\n",
    "    batch_size=256,\n",
    "    gamma=0.99,\n",
    "    exploration_mu=0.0,\n",
    "    exploration_theta=0.15,\n",
    "    exploration_sigma=0.20,\n",
    "    seed=np.random.randint(1000),\n",
    "    update_every=1, \n",
    "    update_repeat=1,\n",
    "    weight_decay=0, \n",
    "    noise_decay=0.99995\n",
    ")\n",
    "\n",
    "print('Running on: ' + device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. It's Turn of Train the Agent\n",
    "\n",
    "Now it's your turn to train the agent to solve the environment!  The enviroment is considered solved when the agent achieves an avarage score of 0.5 points over 100 episodes.\n",
    "While training the agent, you can see the progress in an online tred.\n",
    "\n",
    "The training follow few main steps, for each episode:\n",
    "- the actor model of the agent run actions on the enviroment with some added noise to ensure exploration. The `OUNoise` Class is used for this purpose.\n",
    "- additional informations from the enviroment are retrieved after the action of the agent (reward, next_state, etc.) and are piled into the **Replay Buffer**. \n",
    "- experiances are extracted from the **Replay Buffer** and used to:\n",
    "    - calculate the TD target\n",
    "    - soft-update the critic model\n",
    "    - soft-update the actor model\n",
    "- The results are collected and showed and if the avarage score passes the 0.5 points and improve from the previous episodes, the models are checkpointed.\n",
    "- At the end of the training the trend with the progress is saved in the folder `Results\\`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got int)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e996d008e5b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Agent Interact with the enviroment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# Enviroment response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Coding\\DRL_Project3-Collaboration_Competition\\agent_jn.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, learn)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Coding\\DRL_Project3-Collaboration_Competition\\agent_jn.py\u001b[0m in \u001b[0;36mtensor\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got int)"
     ]
    }
   ],
   "source": [
    "episodes = 2000\n",
    "steps = 2000\n",
    "\n",
    "scores = deque(maxlen=100)\n",
    "PScores = []\n",
    "\n",
    "# Score Trend Initializaiton\n",
    "plot = Plotting(\n",
    "    title ='Learning Process',\n",
    "    y_label = 'Score',\n",
    "    x_label = 'Episode #',\n",
    "    x_range = 250,\n",
    ")\n",
    "plot.show()\n",
    "\n",
    "# Progress bar o terminal\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "last_saved = 0\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # Reset the OUNoise state\n",
    "    agent.reset_episode()\n",
    "    \n",
    "    # Enviroment initialization and initial state\n",
    "    env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    score = np.zeros(num_agents)\n",
    "    for step_i in range(steps):\n",
    "        \n",
    "        # Agent Interact with the enviroment\n",
    "        action = agent.act(states.shape[0])\n",
    "        \n",
    "        # Enviroment response\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        \n",
    "        #####next_state, reward, done = env.step(action.reshape(2,-1))\n",
    "        \n",
    "        # Interactions are collected in the replay buffer\n",
    "        replay_buffer.add(states.shape[0], actions.shape[0], np.max(rewards), next_states.shape[0], np.max(dones))\n",
    "        \n",
    "        \n",
    "        # Agent learn step\n",
    "        agent.learn_step(states.shape[0], actions.shape[0], np.max(rewards), next_states.shape[0], np.max(dones))\n",
    "        \n",
    "        # Shift to next experiance step\n",
    "        states = next_states\n",
    "        score += rewards                     # Episode score builtup\n",
    "        \n",
    "        # Episode end condition\n",
    "        if done.any():\n",
    "            break\n",
    "    # collect  episode results\n",
    "    scores.append(score.max())\n",
    "    mean = np.sum(scores)/100\n",
    "    if (episode+1)%50 ==0 :\n",
    "        print(\"Episode: {0:d}, Score: {1:f}\".format(episode+1,mean))\n",
    "    timer.update(episode+1)\n",
    "    #summary = f'Episode: {episode+1}/{episodes}, Steps: {agent.it:d}, Noise: {agent.noise_scale:.2f}, Score Agt. #1: {score[0]:.2f}, Score Agt. #2: {score[1]:.2f}'\n",
    "    PScores.append(mean)\n",
    "    plot.Update(list(range(episode+1)),PScores)\n",
    "    #summary += f', Score: {mean:.3f}'\n",
    "    # writer.add_scalar('data/score', mean, ep_i)\n",
    "    if mean > 0.50 and mean > last_saved:\n",
    "        last_saved = mean\n",
    "        agent.save('saved/trained_model.ckpt')\n",
    "    \n",
    "timer.finish()\n",
    "\n",
    "# Save Training Trend\n",
    "end_plot = Plotting(\n",
    "    title ='Learning Process',\n",
    "    y_label = 'Score',\n",
    "    x_label = 'Episode #',\n",
    "    x_values = list(range(episode-(100-2))),\n",
    "    y_values = PScores\n",
    ")\n",
    "end_plot.save('Results/Training.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. See the Agent Performs!\n",
    "\n",
    "Now that the agent is trained you can see it in action!\n",
    "\n",
    "After loading the checkpoint an episode is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
